This is an excellent project that highlights several in-demand skills in data engineering and back-end development. This README focuses specifically on the back-end components, architecture, and deployment, using the Python/Django stack described in the document.

**üöÄ Cosmari Vehicle Data Integration Platform (Back-End)**

This repository contains the back-end architecture for the Cosmari Vehicle Data Integration Platform, developed in collaboration with Cosmari S.r.l. as part of the Master's Degree in Computer Science at the University of Camerino.

The system's core function is to centralize, process, and analyze heterogeneous data generated by Cosmari's vehicle fleet (garbage collection trucks and street-sweeping vehicles) into a unified platform for operational optimization.

**üíª Technical Stack**

Back-End (Python & Django): Chosen for its rich ecosystem, modularity, and seamless integration capabilities with Airflow and Elasticsearch.

Data Orchestration (Apache Airflow): Automates and schedules the entire Extract, Transform, Load (ETL) data pipeline via Directed Acyclic Graphs (DAGs).

Search & Storage (Elasticsearch): Provides fast, scalable search and storage for data indexing and real-time query capabilities.

Relational DB (PostgreSQL): Used as the primary relational database for handling highly structured data and supporting the application's robust features.

Security (JSON Web Tokens - JWT): Implemented for secure Authentication and Authorization of protected endpoints.


**üèóÔ∏è Architecture and Data Flow**

The back-end utilizes a modular architecture (Django Apps) and a structured ETL pipeline to ensure scalability, efficiency, and maintainability.

Extraction: Scheduled Airflow DAGs initiate API calls (using the Python requests library) to various external providers (Axitea, Bucher) to retrieve raw data.

Transformation: Data undergoes extensive cleaning, normalization, format conversion, and validation to ensure consistency across all sources.

Loading: The standardized data is indexed and stored in Elasticsearch for optimal querying and real-time visualization on the dashboard.

The system also integrates PostgreSQL for structured, transactional data storage and utilizes dedicated API integration apps to manage diverse formatsand authentication mechanisms required by external services.


**‚öôÔ∏è Setup**

To set up the back-end environment locally, you'll need Docker and Docker Compose installed.

-Docker
-Docker Compose (or equivalent containerization tools)
-Python 3.13
